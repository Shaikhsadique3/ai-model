from fastapi import FastAPI, HTTPException, Depends, Header
from fastapi.security import APIKeyHeader
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import pandas as pd
import numpy as np
import joblib
import sqlite3
from typing import Optional, Dict, List
import math

# Initialize FastAPI app
app = FastAPI(
    title="Churn Prediction API",
    description="API for predicting customer churn using machine learning",
    version="1.0.0"
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Load the trained model (version 2)
model = joblib.load('churnaizer_model_v2.pkl')

# Get feature importance from the model
feature_importances = None
if hasattr(model, 'feature_importances_'):
    feature_importances = model.feature_importances_

# API Key header
API_KEY_HEADER = APIKeyHeader(name="X-API-Key")

# Input data model
class CustomerData(BaseModel):
    days_since_signup: int
    monthly_revenue: float
    subscription_plan: str
    number_of_logins_last30days: int
    active_features_used: int
    support_tickets_opened: int
    last_payment_status: str

# Verify API key
async def verify_api_key(api_key: str = Depends(API_KEY_HEADER)):
    conn = sqlite3.connect('api_keys.db')
    cursor = conn.cursor()
    cursor.execute('SELECT * FROM api_keys WHERE key = ?', (api_key,))
    result = cursor.fetchone()
    conn.close()
    
    if not result:
        raise HTTPException(
            status_code=401,
            detail="Invalid API key"
        )
    return api_key

def estimate_time_to_churn(probability: float, churn_prediction: bool) -> Optional[int]:
    """Estimate time to churn based on probability and churn prediction."""
    if not churn_prediction:
        return None # Mark as null if not predicted to churn

    if probability > 0.9:
        return 3
    elif probability > 0.8:
        return 7
    elif probability > 0.7:
        return 14
    elif probability > 0.6:
        return 21
    else:
        return 30 # 30+ days

def identify_churn_reasons(customer_data: dict) -> str:
    """Identify top reasons for potential churn based on user behavior."""
    reasons = []

    if customer_data['number_of_logins_last30days'] < 3:
        reasons.append("Low login activity")
    if customer_data['active_features_used'] < 2:
        reasons.append("Low feature usage")
    if customer_data['support_tickets_opened'] > 2:
        reasons.append("Multiple support issues")
    if customer_data['last_payment_status'] == 'Failed':
        reasons.append("Payment issue")
    if customer_data['subscription_plan'] == 'Free Trial' and customer_data['days_since_signup'] < 7:
        reasons.append("Incomplete onboarding")

    if not reasons:
        return "No specific reason identified based on current heuristics."
    else:
        return " and ".join(reasons)

@app.post("/predict", dependencies=[Depends(verify_api_key)])
async def predict_churn(customer: CustomerData):
    try:
        # Convert input data to DataFrame
        customer_dict = {
            'days_since_signup': customer.days_since_signup,
            'monthly_revenue': customer.monthly_revenue,
            'subscription_plan': customer.subscription_plan,
            'number_of_logins_last30days': customer.number_of_logins_last30days,
            'active_features_used': customer.active_features_used,
            'support_tickets_opened': customer.support_tickets_opened,
            'last_payment_status': customer.last_payment_status
        }
        
        input_data = pd.DataFrame([customer_dict])

        # Encode categorical variables
        # Ensure all expected columns are present after one-hot encoding
        # This part needs to be robust to handle cases where not all categories are present in input_data
        # For simplicity, we'll assume the model was trained with all possible categories
        # and the input_encoded will have the same columns as the training data.
        # In a real-world scenario, you'd use a pre-fitted OneHotEncoder.
        input_encoded = pd.get_dummies(input_data)

        # Align columns with training data columns if necessary
        # This is a placeholder. In a real application, you'd save the columns from training
        # and ensure the prediction input has the same columns in the same order.
        # For now, we'll just use the columns generated by get_dummies on the input.
        
        # Make prediction
        prediction = model.predict(input_encoded)[0]
        probability = model.predict_proba(input_encoded)[0][1]
        
        # Estimate time to churn
        expected_churn_in_days = estimate_time_to_churn(probability, bool(prediction))
        
        # Identify reasons for churn
        reason = identify_churn_reasons(customer_dict)

        return {
            "churn_prediction": bool(prediction),
            "churn_probability": float(probability),
            "message": "High risk of churn" if prediction == 1 else "Low risk of churn",
            "reason": reason,
            "expected_churn_in_days": expected_churn_in_days
        }

    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Prediction error: {str(e)}"
        )

@app.get("/health")
async def health_check():
    return {"status": "healthy", "model": "loaded"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)